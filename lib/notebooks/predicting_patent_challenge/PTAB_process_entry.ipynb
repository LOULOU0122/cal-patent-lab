{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidjwiner/anaconda3/envs/py27/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/davidjwiner/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pandas import DataFrame\n",
    "from sqlalchemy import create_engine\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "from sqlalchemy import create_engine\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting invalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we pull down all of the patents that have ever been brought before the PTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>patent_id</th>\n",
       "      <th>invalidated</th>\n",
       "      <th>denied</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>decision_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBM2012-00001</td>\n",
       "      <td>6553350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-16</td>\n",
       "      <td>2013-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBM2012-00002</td>\n",
       "      <td>6064970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-16</td>\n",
       "      <td>2013-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBM2012-00003</td>\n",
       "      <td>8140358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-16</td>\n",
       "      <td>2013-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBM2012-00004</td>\n",
       "      <td>6064970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-16</td>\n",
       "      <td>2013-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBM2012-00005</td>\n",
       "      <td>6675151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-21</td>\n",
       "      <td>2013-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CBM2012-00007</td>\n",
       "      <td>5361201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-19</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CBM2012-00010</td>\n",
       "      <td>7124088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>2013-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CBM2012-00011</td>\n",
       "      <td>7124088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>2013-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CBM2013-00001</td>\n",
       "      <td>7877269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-10-03</td>\n",
       "      <td>2013-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CBM2013-00002</td>\n",
       "      <td>7877269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-10-02</td>\n",
       "      <td>2013-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CBM2013-00003</td>\n",
       "      <td>8090598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>2013-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CBM2013-00004</td>\n",
       "      <td>8090598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>2013-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CBM2013-00005</td>\n",
       "      <td>7941357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>2013-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CBM2013-00008</td>\n",
       "      <td>6438526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>2013-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CBM2013-00009</td>\n",
       "      <td>8140358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>2013-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CBM2013-00013</td>\n",
       "      <td>8037158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-03-22</td>\n",
       "      <td>2013-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CBM2013-00014</td>\n",
       "      <td>6625582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-03-29</td>\n",
       "      <td>2013-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CBM2013-00015</td>\n",
       "      <td>5862223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-04-02</td>\n",
       "      <td>2013-07-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CBM2013-00016</td>\n",
       "      <td>8346637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-04-23</td>\n",
       "      <td>2013-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CBM2013-00017</td>\n",
       "      <td>6834282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-23</td>\n",
       "      <td>2013-10-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          case_id patent_id  invalidated  denied filing_date decision_date\n",
       "0   CBM2012-00001   6553350          1.0     0.0  2012-09-16    2013-01-09\n",
       "1   CBM2012-00002   6064970          1.0     0.0  2012-09-16    2013-01-25\n",
       "2   CBM2012-00003   8140358          1.0     0.0  2012-09-16    2013-02-12\n",
       "3   CBM2012-00004   6064970          1.0     0.0  2012-09-16    2013-01-25\n",
       "4   CBM2012-00005   6675151          1.0     0.0  2012-09-21    2013-01-23\n",
       "5   CBM2012-00007   5361201          1.0     0.0  2012-09-19    2013-01-31\n",
       "6   CBM2012-00010   7124088          1.0     0.0  2012-09-29    2013-02-25\n",
       "7   CBM2012-00011   7124088          0.0     1.0  2012-09-29    2013-02-25\n",
       "8   CBM2013-00001   7877269          0.0     1.0  2012-10-03    2013-02-27\n",
       "9   CBM2013-00002   7877269          1.0     0.0  2012-10-02    2013-02-27\n",
       "10  CBM2013-00003   8090598          0.0     1.0  2012-10-15    2013-03-15\n",
       "11  CBM2013-00004   8090598          1.0     0.0  2012-10-15    2013-03-15\n",
       "12  CBM2013-00005   7941357          NaN     NaN  2012-10-15    2013-03-29\n",
       "13  CBM2013-00008   6438526          NaN     NaN  2012-11-14    2013-06-24\n",
       "14  CBM2013-00009   8140358          1.0     0.0  2012-11-20    2013-03-28\n",
       "15  CBM2013-00013   8037158          1.0     0.0  2013-03-22    2013-09-19\n",
       "16  CBM2013-00014   6625582          1.0     0.0  2013-03-29    2013-09-20\n",
       "17  CBM2013-00015   5862223          0.0     1.0  2013-04-02    2013-07-25\n",
       "18  CBM2013-00016   8346637          0.0     1.0  2013-04-23    2013-10-07\n",
       "19  CBM2013-00017   6834282          NaN     NaN  2013-04-23    2013-10-24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connecting to the db\n",
    "host_db = \"cal-patent-lab.chhaitskv8dz.us-west-2.rds.amazonaws.com\"\n",
    "username = \"teamrocket\"\n",
    "password = \"teamrocket\"\n",
    "db = \"teamrocket\"\n",
    "\n",
    "engine = create_engine(\"mysql://{}:{}@{}/{}\".format(\n",
    "    username, password, host_db, db))\n",
    "connection = engine.connect()\n",
    "q = \"SELECT case_id, patent_id, invalidated, denied, filing_date, decision_date FROM ptab_cases\"\n",
    "ptab_patents = pd.read_sql(q, engine)\n",
    "connection.close()\n",
    "\n",
    "ptab_patents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we want to grab the claims text associated with each patent and insert it into the dataframe\n",
    "\n",
    "host_db = \"rosencrantz.berkeley.edu\"\n",
    "username = \"uspto\"\n",
    "password = \"ferrisbueller\"\n",
    "db = \"uspto\"\n",
    "\n",
    "engine = create_engine(\"mysql://{}:{}@{}/{}\".format(\n",
    "    username, password, host_db, db))\n",
    "connection = engine.connect()\n",
    "\n",
    "patent_id = int(ptab_patents.patent_id[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patent 0\n",
      "Processing patent 100\n",
      "Processing patent 200\n",
      "Processing patent 300\n",
      "Processing patent 400\n",
      "Processing patent 500\n",
      "Processing patent 600\n",
      "Processing patent 700\n",
      "Processing patent 800\n",
      "Processing patent 900\n",
      "Processing patent 1000\n",
      "Processing patent 1100\n",
      "Processing patent 1200\n",
      "Processing patent 1300\n",
      "Processing patent 1400\n",
      "Processing patent 1500\n",
      "Processing patent 1600\n",
      "Processing patent 1700\n",
      "Processing patent 1800\n",
      "Processing patent 1900\n",
      "Processing patent 2000\n",
      "Processing patent 2100\n",
      "Processing patent 2200\n",
      "Processing patent 2300\n",
      "Processing patent 2400\n",
      "Processing patent 2500\n",
      "Processing patent 2600\n",
      "Processing patent 2700\n",
      "Processing patent 2800\n",
      "Processing patent 2900\n",
      "Processing patent 3000\n",
      "Processing patent 3100\n",
      "Processing patent 3200\n",
      "Processing patent 3300\n",
      "Processing patent 3400\n",
      "Processing patent 3500\n",
      "Processing patent 3600\n",
      "Processing patent 3700\n",
      "Processing patent 3800\n",
      "Processing patent 3900\n",
      "Processing patent 4000\n",
      "Processing patent 4100\n",
      "Processing patent 4200\n",
      "Processing patent 4300\n",
      "Processing patent 4400\n",
      "Processing patent 4500\n",
      "Processing patent 4600\n",
      "Processing patent 4700\n",
      "Processing patent 4800\n",
      "Processing patent 4900\n",
      "Processing patent 5000\n",
      "Processing patent 5100\n",
      "Processing patent 5200\n",
      "Processing patent 5300\n",
      "Processing patent 5400\n",
      "Processing patent 5500\n",
      "Processing patent 5600\n",
      "Processing patent 5700\n",
      "Processing patent 5800\n",
      "Processing patent 5900\n",
      "Processing patent 6000\n",
      "Processing patent 6100\n",
      "Processing patent 6200\n",
      "Processing patent 6300\n",
      "Processing patent 6400\n"
     ]
    }
   ],
   "source": [
    "# Inserting concatenated claim text into ptab_patents\n",
    "\n",
    "found = list()\n",
    "for idx, patent_id in enumerate(ptab_patents.patent_id):\n",
    "    q = \"SELECT text from uspto.claim where patent_id = '{}'\".format(patent_id)\n",
    "    claims = pd.read_sql(q, engine)\n",
    "    claims_concat = claims.text.str.cat()\n",
    "    ptab_patents.loc[ptab_patents.patent_id == '{}'.format(patent_id), 'claim_text'] = claims_concat\n",
    "    found.append(not claims.empty)\n",
    "    if idx % 100 == 0:\n",
    "        print(\"Processing patent {}\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ptab_patents.to_pickle('ptab_patents.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pickled data, deduplicating patents, and running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptab_patents = pd.read_pickle('ptab_patents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# De-duplicating patents\n",
    "\n",
    "df = ptab_patents.dropna(axis=0, how='any')\n",
    "mask = (ptab_patents.claim_text.str.len() > 1)\n",
    "df = df.loc[mask]\n",
    "dups = df.duplicated(subset='claim_text', keep='first')\n",
    "df = df.loc[~dups]\n",
    "\n",
    "\n",
    "X = df.claim_text.as_matrix()\n",
    "y = df.invalidated.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def train_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X, y, test_size=0.2, random_state=20)\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(X)\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "\n",
    "    svc_class = LinearSVC(C=1)\n",
    "    model = svc_class.fit(X_train, y_train)\n",
    "    \n",
    "#     logistic_class = LogisticRegression(dual=True)\n",
    "#     model = logistic_class.fit(X_train, y_train)\n",
    "\n",
    "    precision = precision_score(y_test, model.predict(X_test))\n",
    "    recall = recall_score(y_test, model.predict(X_test))\n",
    "\n",
    "    print(\"Training accuracy is {0}\".format(model.score(X_train, y_train)))\n",
    "    print(\"Testing accuracy is {0}\".format(model.score(X_test, y_test)))\n",
    "    print(\"Precision is {0}\".format(precision))\n",
    "    print(\"Recall is {0}\".format(recall))\n",
    "\n",
    "    vals = df.invalidated.value_counts()\n",
    "    print(\"Outcome of guessing is {}\".format(float(vals[0])/float(sum(vals))))\n",
    "    print(\"N is {0}\".format(sum(vals)))\n",
    "    \n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.986923076923\n",
      "Testing accuracy is 0.683076923077\n",
      "Precision is 0.709677419355\n",
      "Recall is 0.729281767956\n",
      "Outcome of guessing is 0.617846153846\n",
      "N is 1625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0), <325x14366 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 31661 stored elements in Compressed Sparse Row format>, array([ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "         1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "         1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,\n",
       "         0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,\n",
       "         1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,\n",
       "         1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "         1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is high when we keep our false negatives low and precision is high when we keep our false positives low. Basically, this classifier is overindexing on precision relative to recall. A next step here would be to look at tradeoffs between precision and recall between different probabalistic classifiers. We probably also want to play with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting denials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 6450 total cases, 3645 have null for their denied status\n"
     ]
    }
   ],
   "source": [
    "null_denied_counts = ptab_patents.denied.notnull().value_counts()\n",
    "print(\"Out of {0} total cases, {1} have null for their denied status\".format(sum(null_denied_counts), null_denied_counts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 6450 total cases, 2487 are on duplicate patents\n"
     ]
    }
   ],
   "source": [
    "dup_patent_counts = ptab_patents.duplicated(subset='patent_id', keep='first').value_counts()\n",
    "print(\"Out of {0} total cases, {1} are on duplicate patents\".format(sum(dup_patent_counts), dup_patent_counts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data cleaning__: Suppose we drop the nulls and duplicate patents (for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First get rid of patents where we cannot find the claim text\n",
    "mask = (ptab_patents.claim_text.str.len() > 1)\n",
    "df = ptab_patents.loc[mask]\n",
    "\n",
    "# Then we get rid of patents with no 'denied' status\n",
    "null_denied = df.denied.isnull()\n",
    "df = df[~null_denied]\n",
    "\n",
    "# Finally, we want to get rid of duplicates\n",
    "dups = df.duplicated(subset='patent_id', keep='first')\n",
    "df = df[~dups]\n",
    "\n",
    "X = df.claim_text.as_matrix()\n",
    "y = df.denied.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.986923076923\n",
      "Testing accuracy is 0.683076923077\n",
      "Precision is 0.709677419355\n",
      "Recall is 0.729281767956\n",
      "Outcome of guessing is 0.617846153846\n",
      "N is 1625\n"
     ]
    }
   ],
   "source": [
    "model, X_test, y_test = train_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: ROC curves, cross-validation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
